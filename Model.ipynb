{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 10, 10, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 5, 5, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 19203     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411,811\n",
      "Trainable params: 411,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Third convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Fourth convolutional layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Fifth convolutional layer\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output for the fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 3 classes.\n",
      "Found 826 images belonging to 3 classes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28576/1073082582.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data augmentation parameters\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory('DownsampledDataset2',\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    batch_size=50)\n",
    "val_generator = train_datagen.flow_from_directory('PreprocessedDataset/val',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  batch_size=50)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1580 images belonging to 3 classes.\n",
      "1580/1580 [==============================] - 13s 8ms/step\n",
      "there were 1044 errors in 1580 tests for an accuracy of  33.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "----------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3531    0.5595    0.4330       563\n",
      "           1     0.4752    0.2059    0.2873       743\n",
      "           2     0.1858    0.2482    0.2125       274\n",
      "\n",
      "    accuracy                         0.3392      1580\n",
      "   macro avg     0.3380    0.3379    0.3109      1580\n",
      "weighted avg     0.3815    0.3392    0.3263      1580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def predictor(test_gen, test_steps):\n",
    "    y_pred= []\n",
    "    y_true=test_gen.labels\n",
    "    classes=list(test_gen.class_indices.keys())\n",
    "    class_count=len(classes)\n",
    "    errors=0\n",
    "    preds=model.predict(test_gen, verbose=1)\n",
    "    tests=len(preds)    \n",
    "    for i, p in enumerate(preds):        \n",
    "        pred_index=np.argmax(p)         \n",
    "        true_index=test_gen.labels[i]  # labels are integer values        \n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors=errors + 1\n",
    "            file=test_gen.filenames[i]            \n",
    "        y_pred.append(pred_index)\n",
    "            \n",
    "    acc=( 1-errors/tests) * 100\n",
    "    print(f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}')\n",
    "    ypred=np.array(y_pred)\n",
    "    ytrue=np.array(y_true)\n",
    "    if class_count <=30:\n",
    "        cm = confusion_matrix(ytrue, ypred )\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    clr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    return errors, tests\n",
    "test_gen = train_datagen.flow_from_directory('PreprocessedDataset/test',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  batch_size=1)\n",
    "errors, tests=predictor(test_gen, len(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "30/30 [==============================] - 26s 671ms/step - loss: 10.4697 - accuracy: 0.5067 - val_loss: 11.8324 - val_accuracy: 0.1610\n",
      "Epoch 2/40\n",
      "11/30 [==========>...................] - ETA: 10s - loss: 5.5247 - accuracy: 0.6109"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28576/2745835670.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\visha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Create sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Load ResNet50 base model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model.add(base_model)\n",
    "\n",
    "# Normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Global average pooling layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# Regularization layer\n",
    "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "# Classifier\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=40,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load VGG19 model with imagenet weights\n",
    "vgg19 = VGG19(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Take output from one of the earlier layers\n",
    "x = vgg19.layers[-3].output\n",
    "\n",
    "# Add additional layers similar to the ones in your Xception model\n",
    "x = Conv2D(filters=1024, kernel_size=3, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = Conv2D(filters=256, kernel_size=3, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = Conv2D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "# Assuming n_class is defined earlier in your code\n",
    "n_class = 10  # You can change this according to your problem\n",
    "\n",
    "x = Conv2D(filters=n_class, kernel_size=3, padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "# Global Average Pooling\n",
    "GAP = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Softmax activation for classification\n",
    "pred = Activation(\"softmax\")(GAP)\n",
    "\n",
    "# Create model\n",
    "vgg19_model = Model(inputs=vgg19.input, outputs=pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
